{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Alunos:\n",
        "\n",
        "*   Andréa Fonseca\n",
        "*   Fábio Cardoso\n",
        "*   Eduardo Leite\n"
      ],
      "metadata": {
        "id": "frGMNtITr-zW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sHpGwKFXK6hP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-gfhMA9hK6hT"
      },
      "outputs": [],
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "d_ff = 2048\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8bbSlXHzK6hT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dc9ab19-fcaa-4c03-c757-3ac3746ee544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50, 32, 512])\n"
          ]
        }
      ],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "# Exemplo\n",
        "max_len = 100\n",
        "pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "# (sequence_length, batch_size, d_model)\n",
        "input_tensor = torch.randn(50, batch_size, d_model)\n",
        "output_tensor = pos_encoding(input_tensor)\n",
        "\n",
        "print(output_tensor.shape)  # Output shape: (sequence_length, batch_size, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dLC6j7u0K6hU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c49b27d-6f7a-4dac-e11a-4d5acd7973f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 50, 512])\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        # Verifica se o número de dimensões do modelo é divisível pelo número de cabeças\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        # Número de dimensões por cabeça\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # Inicializa as camadas lineares para Q, K e V\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        # Calcula os scores fazendo o produto escalar entre Q e K e dividindo pela raiz quadrada de d_k\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Se a máscara for fornecida, aplica a máscara para os scores\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        # Calcula a softmax nos scores\n",
        "        attention = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        # Multiplica a matriz de atenção pelo valor V\n",
        "        output = torch.matmul(attention, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Divide a última dimensão em (num_heads, d_k)\n",
        "        N, seq_len, d_model = x.size()\n",
        "        return x.view(N, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        # Inverte a operação de split_heads\n",
        "        N, _, seq_len, _ = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(N, seq_len, self.num_heads * self.d_k)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        N = query.shape[0]\n",
        "        query_len, key_len, value_len = query.shape[1], key.shape[1], value.shape[1]\n",
        "\n",
        "        # Passa os valores de Q, K e V pela camada linear\n",
        "        Q = self.split_heads(self.W_q(query))\n",
        "        K = self.split_heads(self.W_k(key))\n",
        "        V = self.split_heads(self.W_v(value))\n",
        "\n",
        "        # Calcula a atenção\n",
        "        attention = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # Combina as cabeças e aplica a camada linear final\n",
        "        output = self.combine_heads(attention)\n",
        "        output = self.W_o(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "multi_head_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "# (batch_size, sequence_length, d_model)\n",
        "query = torch.randn(batch_size, 50, d_model)\n",
        "key = torch.randn(batch_size, 50, d_model)\n",
        "value = torch.randn(batch_size, 50, d_model)\n",
        "\n",
        "output = multi_head_attn(query, key, value)\n",
        "\n",
        "print(output.shape)  # Output shape: (batch_size, sequence_length, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GhovNcoxK6hV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d66b251a-4bb7-4550-b9eb-862a898357e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 50, 512])\n"
          ]
        }
      ],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "\n",
        "d_model = 512\n",
        "d_ff = 2048\n",
        "ffn = FeedForward(d_model, d_ff)\n",
        "\n",
        "# (batch_size, sequence_length, d_model)\n",
        "input_tensor = torch.randn(32, 50, d_model)\n",
        "\n",
        "output = ffn(input_tensor)\n",
        "\n",
        "print(output.shape)  # Output shape: (batch_size, sequence_length, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KU2EX-wmK6hW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dafe0441-eac3-4ef1-8d6d-86e1d067e60f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 50, 512])\n"
          ]
        }
      ],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.attention(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout(ffn_output))\n",
        "        return x\n",
        "\n",
        "\n",
        "encoder_layer = EncoderLayer(d_model, num_heads, d_ff)\n",
        "\n",
        "# (batch_size, sequence_length, d_model)\n",
        "input_tensor = torch.randn(32, 50, d_model)\n",
        "\n",
        "output = encoder_layer(input_tensor)\n",
        "\n",
        "print(output.shape)  # Output shape: (batch_size, sequence_length, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7kOiHw-tK6hW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0105f5d5-eed6-47d0-ed01-e091d9bff39f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 100, 512])\n"
          ]
        }
      ],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, src_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Embedding + positional encoding + dropout\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        # x = self.dropout(x)\n",
        "\n",
        "        # Passa a entrada por cada camada do encoder\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "src_vocab_size = 1000\n",
        "num_layers = 6\n",
        "max_len = 100\n",
        "\n",
        "encoder = Encoder(src_vocab_size, d_model, num_heads, num_layers, d_ff, max_len)\n",
        "\n",
        "# (batch_size, sequence_length)\n",
        "input_seq = torch.randint(0, src_vocab_size, (32, 100))\n",
        "\n",
        "output = encoder(input_seq)\n",
        "\n",
        "print(output.shape)  # Output shape: (batch_size, sequence_length, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_7479xkAK6hX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b5baa56-9075-4e56-d823-ae959651ef23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 50, 512])\n"
          ]
        }
      ],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.cross_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask=None, trg_mask=None):\n",
        "        # Self-attention na sequência de destino\n",
        "        self_attn_output = self.self_attention(x, x, x, trg_mask)\n",
        "        x = self.norm1(x + self.dropout(self_attn_output))\n",
        "\n",
        "        # Cross-attention entre a saída do self-attention e a saída do encoder\n",
        "        cross_attn_output = self.cross_attention(x, enc_out, enc_out, src_mask)\n",
        "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
        "\n",
        "        # Feed-forward\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm3(x + self.dropout(ffn_output))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "decoder_layer = DecoderLayer(d_model, num_heads, d_ff)\n",
        "\n",
        "# (batch_size, sequence_length, d_model)\n",
        "input_tensor = torch.randn(32, 50, d_model)\n",
        "enc_out = torch.randn(32, 50, d_model)\n",
        "\n",
        "output = decoder_layer(input_tensor, enc_out)\n",
        "\n",
        "print(output.shape)  # Output shape: (batch_size, sequence_length, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XPGz4uUZK6hX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8c179fc-0007-403c-d2dc-907ea12e8a4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 100, 1000])\n"
          ]
        }
      ],
      "source": [
        "# Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, trg_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(trg_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.fc_out = nn.Linear(d_model, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask=None, trg_mask=None):\n",
        "        # Embedding + positional encoding + dropout\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Passa a entrada por cada camada do decoder\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out, src_mask, trg_mask)\n",
        "\n",
        "        out = self.fc_out(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "trg_vocab_size = 1000\n",
        "num_layers = 6\n",
        "max_len = 100\n",
        "\n",
        "decoder = Decoder(trg_vocab_size, d_model, num_heads, num_layers, d_ff, max_len)\n",
        "\n",
        "# (batch_size, sequence_length)\n",
        "trg_seq = torch.randint(0, trg_vocab_size, (32, 100))\n",
        "enc_out = torch.randn(32, 100, d_model)\n",
        "\n",
        "output = decoder(trg_seq, enc_out)\n",
        "\n",
        "print(output.shape)  # Output shape: (batch_size, sequence_length, trg_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "e9LGk-iCK6hY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "292627c1-3587-42b0-9bf5-be130990fd81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 100, 1000])\n"
          ]
        }
      ],
      "source": [
        "# Transformer Completo\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, d_ff, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_heads, num_encoder_layers, d_ff, max_len, dropout)\n",
        "        self.decoder = Decoder(trg_vocab_size, d_model, num_heads, num_decoder_layers, d_ff, max_len, dropout)\n",
        "\n",
        "    def generate_mask(self, src, trg):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        trg_mask = (trg != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = trg.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
        "        trg_mask = trg_mask & nopeak_mask\n",
        "        return src_mask, trg_mask\n",
        "\n",
        "    def forward(self, src, trg, src_mask=None, trg_mask=None):\n",
        "        src_mask, trg_mask = self.generate_mask(src, trg)\n",
        "        enc_out = self.encoder(src, src_mask)\n",
        "        out = self.decoder(trg, enc_out, src_mask, trg_mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "src_vocab_size = 1000\n",
        "trg_vocab_size = 1000\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_encoder_layers = 6\n",
        "num_decoder_layers = 6\n",
        "d_ff = 2048\n",
        "max_len = 100\n",
        "\n",
        "transformer = Transformer(src_vocab_size, trg_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, d_ff, max_len)\n",
        "\n",
        "# (batch_size, sequence_length)\n",
        "src_seq = torch.randint(0, src_vocab_size, (batch_size, 100))\n",
        "trg_seq = torch.randint(0, trg_vocab_size, (batch_size, 100))\n",
        "\n",
        "output = transformer(src_seq, trg_seq)\n",
        "\n",
        "print(output.shape)  # Output shape: (batch_size, target_sequence_length, trg_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "J95ikR3wK6hY"
      },
      "outputs": [],
      "source": [
        "# Gerando as máscaras de forma separada\n",
        "\n",
        "def create_padding_mask(seq):\n",
        "    return (seq != 0).unsqueeze(1).unsqueeze(2).type(torch.uint8)  # Cria uma máscara para posições de preenchimento\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = (1 - torch.triu(torch.ones(size, size), diagonal=1)).type(torch.uint8)\n",
        "    return mask  # Cria uma máscara triangular para impedir a atenção em tokens futuros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JCZPgsjUK6hZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e15b05b-e475-4478-9772-98127e2e286f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[1, 1, 0, 1, 0]]]], dtype=torch.uint8)\n"
          ]
        }
      ],
      "source": [
        "seq = torch.tensor([[1, 2, 0, 4, 0]])\n",
        "padding_mask = create_padding_mask(seq)\n",
        "print(padding_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ue1jyNFWK6hZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b281905-62a1-46bf-c101-01c4a06e43be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 0, 0, 0, 0],\n",
            "        [1, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 1]], dtype=torch.uint8)\n"
          ]
        }
      ],
      "source": [
        "look_ahead_mask = create_look_ahead_mask(5)\n",
        "print(look_ahead_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVIGJktXK6hZ"
      },
      "source": [
        "## Exercícios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu5DMBWfK6ha"
      },
      "source": [
        "### Exercício 1\n",
        "Implemente um módulo que utilize apenas o módulo Encoder para a classificação de texto em `num_classes` classes. Para a obtenção do vetor de embedding de toda a sequência que será enviado para a cabeça de classificação, faça um pooling de média através da dimensão de sequência."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TtTAN9NxK6ha"
      },
      "outputs": [],
      "source": [
        "class TextClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, src_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout=0.1, num_classes=5):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "      # Embedding + positional encoding + dropout\n",
        "      x = self.embedding(x)\n",
        "      x = self.positional_encoding(x)\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # Passa a entrada por cada camada do encoder\n",
        "      for layer in self.layers:\n",
        "          x = layer(x, mask)\n",
        "\n",
        "      x = x.mean(dim=1)\n",
        "      out = self.fc(x) # cabeça de classificação\n",
        "\n",
        "      return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = 1000\n",
        "num_layers = 6\n",
        "max_len = 100\n",
        "\n",
        "classifier = TextClassifier(src_vocab_size, d_model, num_heads, num_layers, d_ff, max_len)\n",
        "\n",
        "# (batch_size, sequence_length)\n",
        "input_seq = torch.randint(0, src_vocab_size, (32, 100))\n",
        "\n",
        "output = classifier(input_seq)\n",
        "\n",
        "print(output.shape)  # Output shape: (batch_size, sequence_length, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7bEIIgSKut1",
        "outputId": "87121c4d-9467-4a07-b5f1-d66788304f60"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9rjIgHdK6hb"
      },
      "source": [
        "### Exercício 2\n",
        "Vamos implementar um modelo baseado em stack de decoders. Uma vez que não é necessário cross-attention, pois não há encoders, utilize o módulo `EncoderLayer`. O tamanho do vocabulário deverá ser de 50257, o tamanho dos embeddings de 768, 12 cabeças de atenção, 12 camadas, dimensão da camada feedforward de 3072 e tamanho máximo de sequência 1024. Em seguida, teste com valores aleatórios simulando uma sequência de tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zJe5dXvhK6hb"
      },
      "outputs": [],
      "source": [
        "# modelo GPT 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5iuvilG_L7B9"
      },
      "outputs": [],
      "source": [
        "class TextGenerator(nn.Module):\n",
        "\n",
        "  def __init__(self, src_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.fc = nn.Linear(d_model, src_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "      # Embedding + positional encoding + dropout\n",
        "      x = self.embedding(x)\n",
        "      x = self.positional_encoding(x)\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # Passa a entrada por cada camada do encoder\n",
        "      for layer in self.layers:\n",
        "          x = layer(x, mask)\n",
        "\n",
        "      x = x.mean(dim=1)\n",
        "      out = self.fc(x)\n",
        "\n",
        "      return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = TextGenerator(src_vocab_size=50257, d_model=768, num_heads=12, num_layers=12, d_ff=3072, max_len=1024)\n",
        "\n",
        "# (batch_size, sequence_length)\n",
        "input_seq = torch.randint(0, src_vocab_size, (32, 100))\n",
        "\n",
        "output = generator(input_seq)\n",
        "\n",
        "print(output.shape)  # Output shape: (batch_size, sequence_length, d_model) # saída é um token para cada batchsize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fad91cc6-77a4-45d7-c3cb-fb5734f31550",
        "id": "38641sWOL7B-"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gko8JNr8P3WM",
        "outputId": "34f8071a-f548-4527-c1b6-7dab41e572c0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.3514,  1.0782, -0.1336,  ..., -0.7700, -0.0453,  0.3262],\n",
            "        [ 0.1835,  1.1149, -0.0069,  ..., -0.7483, -0.2542,  0.5148],\n",
            "        [ 0.3176,  0.9749,  0.1516,  ..., -0.6456, -0.2467,  0.5769],\n",
            "        ...,\n",
            "        [-0.0269,  0.4731, -0.0146,  ..., -0.2856,  0.1893,  0.3490],\n",
            "        [ 0.1523,  0.4903,  0.1708,  ..., -0.4649,  0.1445,  0.5909],\n",
            "        [ 0.2469,  0.5479,  0.3391,  ..., -0.4119,  0.0468,  0.8635]],\n",
            "       grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}