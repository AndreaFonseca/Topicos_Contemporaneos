{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Alunos:\n",
        "\n",
        "*   AndrÃ©a Fonseca\n",
        "*   FÃ¡bio Cardoso\n",
        "*   Eduardo Leite\n",
        "\n"
      ],
      "metadata": {
        "id": "85yaXGyf-nz6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SrQNWihmgfr3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c6f9c49-1ff2-4558-f9aa-95009669fb95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FarXo1sDgfr5"
      },
      "source": [
        "## Modelos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai langchain_core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KccpOKOXhSnw",
        "outputId": "f8ae9b9e-c942-43e3-e768-f839b8cdc631"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.1.22-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain_core\n",
            "  Downloading langchain_core-0.2.34-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting openai<2.0.0,>=1.40.0 (from langchain_openai)\n",
            "  Downloading openai-1.42.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (6.0.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain_core)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.75 (from langchain_core)\n",
            "  Downloading langsmith-0.1.104-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (2.8.2)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain_core)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (4.12.2)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain_core)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.75->langchain_core)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.75->langchain_core)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m593.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain_core) (2.32.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.7.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain_openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.66.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (2.20.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain_core) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain_core)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain_core)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (2.0.7)\n",
            "Downloading langchain_openai-0.1.22-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.34-py3-none-any.whl (393 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m393.9/393.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.1.104-py3-none-any.whl (149 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m149.1/149.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.42.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, jiter, h11, tiktoken, jsonpatch, httpcore, httpx, openai, langsmith, langchain_core, langchain_openai\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jiter-0.5.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain_core-0.2.34 langchain_openai-0.1.22 langsmith-0.1.104 openai-1.42.0 orjson-3.10.7 tenacity-8.5.0 tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Tz7oLqIygfr6"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LP0FoBbngfr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e230dc8-b08f-4f0a-e083-43c1267c5368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='OlÃ¡! Como posso ajudar vocÃª hoje?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 8, 'total_tokens': 16}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None} id='run-11c81603-9932-4cc1-bc5b-0c347efeb461-0' usage_metadata={'input_tokens': 8, 'output_tokens': 8, 'total_tokens': 16}\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(\"OlÃ¡\")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IUWuYLvZh0nB",
        "outputId": "629fb86e-7468-4a20-e53f-f242c6fc4628"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'OlÃ¡! Como posso ajudar vocÃª hoje?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWDyyET2gfr7"
      },
      "source": [
        "## Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sg6w6yugfr7"
      },
      "source": [
        "### Templates Simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EGDckpPggfr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc0bcb1-6899-4c80-981e-3affaa6c39e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[HumanMessage(content='Traduza o seguinte texto para portuguÃªs: He would have gone to the gym, if it was not raining.')]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(\"Traduza o seguinte texto para portuguÃªs: {texto}\")\n",
        "\n",
        "#runnable\n",
        "prompt = prompt_template.invoke({\"texto\": \"He would have gone to the gym, if it was not raining.\"})\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JnyqmH87gfr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb5cdc59-6d7f-42ce-9471-8ec668b332d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ele teria ido Ã  academia, se nÃ£o estivesse chovendo.\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hixe5666gfr8"
      },
      "source": [
        "### Templates de Mensagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tW0156gIgfr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6c00c4c-754a-4e8d-a0dd-c7acdf4d3265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='OlÃ¡, como vocÃª estÃ¡?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 36, 'total_tokens': 42}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None} id='run-3820a454-05a9-4eea-8766-d232d55d0d7f-0' usage_metadata={'input_tokens': 36, 'output_tokens': 6, 'total_tokens': 42}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"VocÃª Ã© um tradutor de inglÃªs para portuguÃªs. Traduza as mensagens que forem enviadas.\"),\n",
        "    HumanMessage(content=\"Hello, how are you?\"),\n",
        "]\n",
        "\n",
        "# messages = [\n",
        "#     (\"system\", \"VocÃª Ã© um tradutor de inglÃªs para portuguÃªs. Traduza as mensagens que forem enviadas.\"),\n",
        "#     (\"human\", \"Hello, how are you?\"),\n",
        "# ]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_X8Ed8tsgfr9"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"VocÃª Ã© um tradutor de {lingua_origem} para {lingua_destino}. Traduza as mensagens que forem enviadas.\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ib9GFYQKgfr9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "971708a6-2f9d-4a6e-aec7-ac5dce492a42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='VocÃª Ã© um tradutor de alemÃ£o para portuguÃªs. Traduza as mensagens que forem enviadas.'), HumanMessage(content='Wie gehts?')]\n"
          ]
        }
      ],
      "source": [
        "prompt = prompt_template.invoke({\n",
        "    \"lingua_origem\": \"alemÃ£o\",\n",
        "    \"lingua_destino\": \"portuguÃªs\",\n",
        "    \"texto\": \"Wie gehts?\"\n",
        "})\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WFNmEoFRgfr9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7175cebe-aa99-4250-bdb9-e39ae41f1f66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Como vocÃª estÃ¡?\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri5Ci9fUgfr9"
      },
      "source": [
        "### Parsers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DG2MjLn-gfr9"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pa2W0Xomgfr-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "207971a8-6e8c-457f-9889-ff1c6390ee84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Como vocÃª estÃ¡?\n"
          ]
        }
      ],
      "source": [
        "output = parser.invoke(response)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz01Te-Agfr-"
      },
      "source": [
        "## Encadeamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1Wib6Ch4gfr-"
      },
      "outputs": [],
      "source": [
        "chain = prompt_template | llm | parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "45-RqOXngfr-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa14cd5f-eaba-4fc1-b629-41a8b56fc48d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Â¡Las playas de Recife tienen tiburones!\n"
          ]
        }
      ],
      "source": [
        "response = chain.invoke({\n",
        "    \"lingua_origem\": \"inglÃªs\",\n",
        "    \"lingua_destino\": \"espanhol\",\n",
        "    \"texto\": \"As praias de Recife tem tubarÃµes!\"\n",
        "})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "N0j9uA-3gfr-"
      },
      "outputs": [],
      "source": [
        "def translate(texto, lingua_origem, lingua_destino):\n",
        "    response = chain.invoke({\n",
        "        \"lingua_origem\": lingua_origem,\n",
        "        \"lingua_destino\": lingua_destino,\n",
        "        \"texto\": texto\n",
        "    })\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3NNxwubagfr-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45ffac25-a1d3-40a8-fa60-b1e848361aac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Â¡Las playas de Recife tienen tiburones!\n"
          ]
        }
      ],
      "source": [
        "output = translate(\"The beaches of Recife have sharks!\", \"inglÃªs\", \"espanhol\")\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lgyhz6LLgfr-"
      },
      "source": [
        "## ExercÃ­cios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igxWL2j4gfr-"
      },
      "source": [
        "### ExercÃ­cio 1\n",
        "Crie uma `chain` que a partir de um tÃ³pico informado pelo usuÃ¡rio, crie uma piada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2EZaD5mfgfr_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "6313f16b-3136-41a9-a809-c45807720c7a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Por que o jogador de futebol foi ao banco?\\n\\nPorque ele queria melhorar sua \"conta\" de gols! âš½ğŸ’°'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(\"Crie uma piada sobre {topic}\")\n",
        "\n",
        "chain = prompt_template | llm | parser\n",
        "\n",
        "chain.invoke({\n",
        "    \"topic\": \"futebol\"\n",
        "})\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3v-KIY3gfr_"
      },
      "source": [
        "### ExercÃ­cio 2\n",
        "Crie uma `chain` que classifique o sentimento de um texto de entrada em positivo, neutro ou negativo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0wBiGAPjgfr_"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"VocÃª deverÃ¡ classificar o sentimento de um {texto_entrada} em positivo, neutro ou negativo. Classifique os textos enviados.\"),\n",
        "        (\"user\", \"{texto_entrada}\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template | llm | parser\n",
        "\n",
        "chain.invoke({\n",
        "    \"texto_entrada\": \"hÃ¡ 23 anos o mundo perdia Aaliyah. A cantora estava voltando das gravaÃ§Ãµes de um clipe quando o aviÃ£o no qual ela estava viajando caiu nas Bahamas logo apÃ³s a decolagem, matando todas as 9 pessoas a bordo.\"\n",
        "})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LQHAncE1BHSC",
        "outputId": "8a31d5fc-4a77-42e6-e98a-28e91ba83a97"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Negativo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "636fYdkogfr_"
      },
      "source": [
        "### ExercÃ­cio 3\n",
        "Crie uma `chain` que gere o cÃ³digo de uma funÃ§Ã£o Python de acordo com a descriÃ§Ã£o do usuÃ¡rio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0wLM3Vkogfr_"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"VocÃª Ã© um programador de Python deve gerar o cÃ³digo de uma funÃ§Ã£o de acordo com a {descricao} do usuÃ¡rio. Gere o cÃ³digo Python conforme a descriÃ§Ã£o enviada.\"),\n",
        "        (\"user\", \"{descricao}\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template | llm | parser\n",
        "\n",
        "response = chain.invoke({\n",
        "    \"descricao\": \"contar palavras em um texto\",\n",
        "})"
      ],
      "metadata": {
        "id": "ImMLxAMQG9Aq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FmKltxZHdob",
        "outputId": "daa45d95-8e4d-4c9a-972d-3162793487eb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aqui estÃ¡ uma funÃ§Ã£o em Python que conta o nÃºmero de palavras em um texto fornecido pelo usuÃ¡rio:\n",
            "\n",
            "```python\n",
            "def contar_palavras(texto):\n",
            "    # Remove espaÃ§os em branco extras e divide o texto em palavras\n",
            "    palavras = texto.strip().split()\n",
            "    # Retorna o nÃºmero de palavras\n",
            "    return len(palavras)\n",
            "\n",
            "# Solicita ao usuÃ¡rio para inserir um texto\n",
            "texto_usuario = input(\"Digite um texto: \")\n",
            "# Conta as palavras no texto e exibe o resultado\n",
            "numero_palavras = contar_palavras(texto_usuario)\n",
            "print(f\"O nÃºmero de palavras no texto Ã©: {numero_palavras}\")\n",
            "```\n",
            "\n",
            "### Como funciona:\n",
            "1. A funÃ§Ã£o `contar_palavras` aceita uma string como argumento.\n",
            "2. Usa `strip()` para remover espaÃ§os em branco no inÃ­cio e no final da string.\n",
            "3. Usa `split()` para dividir o texto em uma lista de palavras, com base em espaÃ§os.\n",
            "4. Retorna o comprimento da lista, que representa o nÃºmero de palavras.\n",
            "5. O programa solicita que o usuÃ¡rio insira um texto e, em seguida, exibe o nÃºmero de palavras contadas. \n",
            "\n",
            "VocÃª pode executar este cÃ³digo em um ambiente Python para ver como funciona.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6jMZX7pgfr_"
      },
      "source": [
        "### ExercÃ­cio 4\n",
        "Crie uma `chain` que explique de forma simplificada um tÃ³pico geral fornecido pelo usuÃ¡rio e, em seguida, traduza a explicaÃ§Ã£o para inglÃªs. Utilize dois templates encadeados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MnSig3i2gfsA"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"VocÃª deverÃ¡ explicar de forma simplificada um {topico_geral} fornecido pelo usuÃ¡rio e, em seguida, traduzir a explicaÃ§Ã£o para inglÃªs.\"),\n",
        "        (\"user\", \"{topico_geral}\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template | llm | parser\n",
        "\n",
        "response = chain.invoke({\n",
        "    \"topico_geral\": \"queimadas pelo Brasil\",\n",
        "})"
      ],
      "metadata": {
        "id": "LwmMFhefIEL1"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgCqv-oiIMYZ",
        "outputId": "72bd4b72-fe8e-45e5-ce37-4d340f527e3b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As queimadas no Brasil referem-se ao ato de colocar fogo em Ã¡reas de vegetaÃ§Ã£o, muitas vezes para limpar terrenos para agricultura ou pastagens. Esse processo pode causar grandes danos ao meio ambiente, incluindo a destruiÃ§Ã£o de habitats, a liberaÃ§Ã£o de gases poluentes e o aumento da temperatura do clima. As queimadas costumam ocorrer principalmente na AmazÃ´nia, mas tambÃ©m em outras regiÃµes do paÃ­s. O controle e a prevenÃ§Ã£o das queimadas sÃ£o importantes para proteger a biodiversidade e combater as mudanÃ§as climÃ¡ticas.\n",
            "\n",
            "---\n",
            "\n",
            "Burning in Brazil refers to the act of setting fire to areas of vegetation, often to clear land for agriculture or pastures. This process can cause significant harm to the environment, including the destruction of habitats, the release of polluting gases, and an increase in climate temperature. Burnings usually occur mainly in the Amazon, but also in other regions of the country. Controlling and preventing burnings is important to protect biodiversity and combat climate change.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRjlzkwZgfsA"
      },
      "source": [
        "### ExercÃ­cio 5 - Desafio\n",
        "Crie uma `chain` que responda perguntas sobre o CESAR School."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"VocÃª deverÃ¡ responder a uma {pergunta} do usuÃ¡rio sobre o CESAR School, como se fosse funcionÃ¡rio do CESAR School. Seja cortez.\"),\n",
        "        (\"user\", \"{pergunta}\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "kgBN6VZBIfjd"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template | llm | parser\n",
        "\n",
        "response = chain.invoke({\n",
        "    \"pergunta\": \"Como faÃ§o para consultar minhas notas no curso de PÃ³s-GraduaÃ§Ã£o?\",\n",
        "})"
      ],
      "metadata": {
        "id": "6AyvXp2cI12-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "I4AcvMdKgfsA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5622c415-2ef6-46f2-dbc0-3828ca0dd84f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OlÃ¡! Para consultar suas notas no curso de PÃ³s-GraduaÃ§Ã£o da CESAR School, vocÃª pode acessar o portal do aluno atravÃ©s do nosso site oficial. Uma vez logado, vocÃª encontrarÃ¡ a seÃ§Ã£o de \"Notas\" ou \"Desempenho AcadÃªmico\", onde poderÃ¡ visualizar suas avaliaÃ§Ãµes.\n",
            "\n",
            "Caso tenha algum problema para acessar sua conta ou precise de mais informaÃ§Ãµes, sinta-se Ã  vontade para entrar em contato com nossa equipe de suporte ao aluno pelo e-mail ou telefone disponÃ­veis no site. Estamos aqui para ajudar!\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}