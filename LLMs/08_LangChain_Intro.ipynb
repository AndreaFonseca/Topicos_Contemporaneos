{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Alunos:\n",
        "\n",
        "*   Andréa Fonseca\n",
        "*   Fábio Cardoso\n",
        "*   Eduardo Leite\n",
        "\n"
      ],
      "metadata": {
        "id": "85yaXGyf-nz6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SrQNWihmgfr3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c6f9c49-1ff2-4558-f9aa-95009669fb95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FarXo1sDgfr5"
      },
      "source": [
        "## Modelos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai langchain_core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KccpOKOXhSnw",
        "outputId": "f8ae9b9e-c942-43e3-e768-f839b8cdc631"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.1.22-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain_core\n",
            "  Downloading langchain_core-0.2.34-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting openai<2.0.0,>=1.40.0 (from langchain_openai)\n",
            "  Downloading openai-1.42.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (6.0.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain_core)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.75 (from langchain_core)\n",
            "  Downloading langsmith-0.1.104-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (2.8.2)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain_core)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (4.12.2)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain_core)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.75->langchain_core)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.75->langchain_core)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m593.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain_core) (2.32.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.7.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain_openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.66.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (2.20.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain_core) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain_core)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain_core)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (2.0.7)\n",
            "Downloading langchain_openai-0.1.22-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.34-py3-none-any.whl (393 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.9/393.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.1.104-py3-none-any.whl (149 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.1/149.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.42.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, jiter, h11, tiktoken, jsonpatch, httpcore, httpx, openai, langsmith, langchain_core, langchain_openai\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jiter-0.5.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain_core-0.2.34 langchain_openai-0.1.22 langsmith-0.1.104 openai-1.42.0 orjson-3.10.7 tenacity-8.5.0 tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Tz7oLqIygfr6"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LP0FoBbngfr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e230dc8-b08f-4f0a-e083-43c1267c5368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Olá! Como posso ajudar você hoje?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 8, 'total_tokens': 16}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None} id='run-11c81603-9932-4cc1-bc5b-0c347efeb461-0' usage_metadata={'input_tokens': 8, 'output_tokens': 8, 'total_tokens': 16}\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(\"Olá\")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IUWuYLvZh0nB",
        "outputId": "629fb86e-7468-4a20-e53f-f242c6fc4628"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Olá! Como posso ajudar você hoje?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWDyyET2gfr7"
      },
      "source": [
        "## Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sg6w6yugfr7"
      },
      "source": [
        "### Templates Simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EGDckpPggfr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc0bcb1-6899-4c80-981e-3affaa6c39e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[HumanMessage(content='Traduza o seguinte texto para português: He would have gone to the gym, if it was not raining.')]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(\"Traduza o seguinte texto para português: {texto}\")\n",
        "\n",
        "#runnable\n",
        "prompt = prompt_template.invoke({\"texto\": \"He would have gone to the gym, if it was not raining.\"})\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JnyqmH87gfr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb5cdc59-6d7f-42ce-9471-8ec668b332d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ele teria ido à academia, se não estivesse chovendo.\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hixe5666gfr8"
      },
      "source": [
        "### Templates de Mensagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tW0156gIgfr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6c00c4c-754a-4e8d-a0dd-c7acdf4d3265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Olá, como você está?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 36, 'total_tokens': 42}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None} id='run-3820a454-05a9-4eea-8766-d232d55d0d7f-0' usage_metadata={'input_tokens': 36, 'output_tokens': 6, 'total_tokens': 42}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Você é um tradutor de inglês para português. Traduza as mensagens que forem enviadas.\"),\n",
        "    HumanMessage(content=\"Hello, how are you?\"),\n",
        "]\n",
        "\n",
        "# messages = [\n",
        "#     (\"system\", \"Você é um tradutor de inglês para português. Traduza as mensagens que forem enviadas.\"),\n",
        "#     (\"human\", \"Hello, how are you?\"),\n",
        "# ]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_X8Ed8tsgfr9"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Você é um tradutor de {lingua_origem} para {lingua_destino}. Traduza as mensagens que forem enviadas.\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ib9GFYQKgfr9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "971708a6-2f9d-4a6e-aec7-ac5dce492a42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='Você é um tradutor de alemão para português. Traduza as mensagens que forem enviadas.'), HumanMessage(content='Wie gehts?')]\n"
          ]
        }
      ],
      "source": [
        "prompt = prompt_template.invoke({\n",
        "    \"lingua_origem\": \"alemão\",\n",
        "    \"lingua_destino\": \"português\",\n",
        "    \"texto\": \"Wie gehts?\"\n",
        "})\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WFNmEoFRgfr9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7175cebe-aa99-4250-bdb9-e39ae41f1f66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Como você está?\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri5Ci9fUgfr9"
      },
      "source": [
        "### Parsers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DG2MjLn-gfr9"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pa2W0Xomgfr-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "207971a8-6e8c-457f-9889-ff1c6390ee84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Como você está?\n"
          ]
        }
      ],
      "source": [
        "output = parser.invoke(response)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz01Te-Agfr-"
      },
      "source": [
        "## Encadeamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1Wib6Ch4gfr-"
      },
      "outputs": [],
      "source": [
        "chain = prompt_template | llm | parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "45-RqOXngfr-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa14cd5f-eaba-4fc1-b629-41a8b56fc48d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Las playas de Recife tienen tiburones!\n"
          ]
        }
      ],
      "source": [
        "response = chain.invoke({\n",
        "    \"lingua_origem\": \"inglês\",\n",
        "    \"lingua_destino\": \"espanhol\",\n",
        "    \"texto\": \"As praias de Recife tem tubarões!\"\n",
        "})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "N0j9uA-3gfr-"
      },
      "outputs": [],
      "source": [
        "def translate(texto, lingua_origem, lingua_destino):\n",
        "    response = chain.invoke({\n",
        "        \"lingua_origem\": lingua_origem,\n",
        "        \"lingua_destino\": lingua_destino,\n",
        "        \"texto\": texto\n",
        "    })\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3NNxwubagfr-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45ffac25-a1d3-40a8-fa60-b1e848361aac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Las playas de Recife tienen tiburones!\n"
          ]
        }
      ],
      "source": [
        "output = translate(\"The beaches of Recife have sharks!\", \"inglês\", \"espanhol\")\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lgyhz6LLgfr-"
      },
      "source": [
        "## Exercícios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igxWL2j4gfr-"
      },
      "source": [
        "### Exercício 1\n",
        "Crie uma `chain` que a partir de um tópico informado pelo usuário, crie uma piada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2EZaD5mfgfr_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "6313f16b-3136-41a9-a809-c45807720c7a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Por que o jogador de futebol foi ao banco?\\n\\nPorque ele queria melhorar sua \"conta\" de gols! ⚽💰'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(\"Crie uma piada sobre {topic}\")\n",
        "\n",
        "chain = prompt_template | llm | parser\n",
        "\n",
        "chain.invoke({\n",
        "    \"topic\": \"futebol\"\n",
        "})\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3v-KIY3gfr_"
      },
      "source": [
        "### Exercício 2\n",
        "Crie uma `chain` que classifique o sentimento de um texto de entrada em positivo, neutro ou negativo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0wBiGAPjgfr_"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Você deverá classificar o sentimento de um {texto_entrada} em positivo, neutro ou negativo. Classifique os textos enviados.\"),\n",
        "        (\"user\", \"{texto_entrada}\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template | llm | parser\n",
        "\n",
        "chain.invoke({\n",
        "    \"texto_entrada\": \"há 23 anos o mundo perdia Aaliyah. A cantora estava voltando das gravações de um clipe quando o avião no qual ela estava viajando caiu nas Bahamas logo após a decolagem, matando todas as 9 pessoas a bordo.\"\n",
        "})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LQHAncE1BHSC",
        "outputId": "8a31d5fc-4a77-42e6-e98a-28e91ba83a97"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Negativo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "636fYdkogfr_"
      },
      "source": [
        "### Exercício 3\n",
        "Crie uma `chain` que gere o código de uma função Python de acordo com a descrição do usuário."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0wLM3Vkogfr_"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Você é um programador de Python deve gerar o código de uma função de acordo com a {descricao} do usuário. Gere o código Python conforme a descrição enviada.\"),\n",
        "        (\"user\", \"{descricao}\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template | llm | parser\n",
        "\n",
        "response = chain.invoke({\n",
        "    \"descricao\": \"contar palavras em um texto\",\n",
        "})"
      ],
      "metadata": {
        "id": "ImMLxAMQG9Aq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FmKltxZHdob",
        "outputId": "daa45d95-8e4d-4c9a-972d-3162793487eb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aqui está uma função em Python que conta o número de palavras em um texto fornecido pelo usuário:\n",
            "\n",
            "```python\n",
            "def contar_palavras(texto):\n",
            "    # Remove espaços em branco extras e divide o texto em palavras\n",
            "    palavras = texto.strip().split()\n",
            "    # Retorna o número de palavras\n",
            "    return len(palavras)\n",
            "\n",
            "# Solicita ao usuário para inserir um texto\n",
            "texto_usuario = input(\"Digite um texto: \")\n",
            "# Conta as palavras no texto e exibe o resultado\n",
            "numero_palavras = contar_palavras(texto_usuario)\n",
            "print(f\"O número de palavras no texto é: {numero_palavras}\")\n",
            "```\n",
            "\n",
            "### Como funciona:\n",
            "1. A função `contar_palavras` aceita uma string como argumento.\n",
            "2. Usa `strip()` para remover espaços em branco no início e no final da string.\n",
            "3. Usa `split()` para dividir o texto em uma lista de palavras, com base em espaços.\n",
            "4. Retorna o comprimento da lista, que representa o número de palavras.\n",
            "5. O programa solicita que o usuário insira um texto e, em seguida, exibe o número de palavras contadas. \n",
            "\n",
            "Você pode executar este código em um ambiente Python para ver como funciona.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6jMZX7pgfr_"
      },
      "source": [
        "### Exercício 4\n",
        "Crie uma `chain` que explique de forma simplificada um tópico geral fornecido pelo usuário e, em seguida, traduza a explicação para inglês. Utilize dois templates encadeados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MnSig3i2gfsA"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Você deverá explicar de forma simplificada um {topico_geral} fornecido pelo usuário e, em seguida, traduzir a explicação para inglês.\"),\n",
        "        (\"user\", \"{topico_geral}\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template | llm | parser\n",
        "\n",
        "response = chain.invoke({\n",
        "    \"topico_geral\": \"queimadas pelo Brasil\",\n",
        "})"
      ],
      "metadata": {
        "id": "LwmMFhefIEL1"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgCqv-oiIMYZ",
        "outputId": "72bd4b72-fe8e-45e5-ce37-4d340f527e3b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As queimadas no Brasil referem-se ao ato de colocar fogo em áreas de vegetação, muitas vezes para limpar terrenos para agricultura ou pastagens. Esse processo pode causar grandes danos ao meio ambiente, incluindo a destruição de habitats, a liberação de gases poluentes e o aumento da temperatura do clima. As queimadas costumam ocorrer principalmente na Amazônia, mas também em outras regiões do país. O controle e a prevenção das queimadas são importantes para proteger a biodiversidade e combater as mudanças climáticas.\n",
            "\n",
            "---\n",
            "\n",
            "Burning in Brazil refers to the act of setting fire to areas of vegetation, often to clear land for agriculture or pastures. This process can cause significant harm to the environment, including the destruction of habitats, the release of polluting gases, and an increase in climate temperature. Burnings usually occur mainly in the Amazon, but also in other regions of the country. Controlling and preventing burnings is important to protect biodiversity and combat climate change.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRjlzkwZgfsA"
      },
      "source": [
        "### Exercício 5 - Desafio\n",
        "Crie uma `chain` que responda perguntas sobre o CESAR School."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Você deverá responder a uma {pergunta} do usuário sobre o CESAR School, como se fosse funcionário do CESAR School. Seja cortez.\"),\n",
        "        (\"user\", \"{pergunta}\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "kgBN6VZBIfjd"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template | llm | parser\n",
        "\n",
        "response = chain.invoke({\n",
        "    \"pergunta\": \"Como faço para consultar minhas notas no curso de Pós-Graduação?\",\n",
        "})"
      ],
      "metadata": {
        "id": "6AyvXp2cI12-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "I4AcvMdKgfsA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5622c415-2ef6-46f2-dbc0-3828ca0dd84f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Olá! Para consultar suas notas no curso de Pós-Graduação da CESAR School, você pode acessar o portal do aluno através do nosso site oficial. Uma vez logado, você encontrará a seção de \"Notas\" ou \"Desempenho Acadêmico\", onde poderá visualizar suas avaliações.\n",
            "\n",
            "Caso tenha algum problema para acessar sua conta ou precise de mais informações, sinta-se à vontade para entrar em contato com nossa equipe de suporte ao aluno pelo e-mail ou telefone disponíveis no site. Estamos aqui para ajudar!\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}